{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed31b489-23e5-408d-87fd-e99ec4db7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0faf38df-6634-4cb9-a2e7-f676810b6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    features = data.columns.to_list()\n",
    "    features.remove('PassengerId')\n",
    "    features.remove('Name')\n",
    "    features.remove('Ticket')\n",
    "    features.remove('Embarked')\n",
    "    features.remove('Parch')\n",
    "    features.remove('Cabin')\n",
    "    features.remove('Pclass')\n",
    "    features.remove('Survived')\n",
    "    x_data = data[features]\n",
    "    y_data = data['Survived']\n",
    "    x_data['Sex'] = x_data['Sex'].apply(lambda x:0 if x=='male' else 1)\n",
    "    x_data['Age'].fillna(x_data['Age'].mean(),inplace=True)\n",
    "    x_data['Fare'].fillna(x_data['Fare'].median(),inplace=True)\n",
    "    x_data = torch.tensor(x_data.to_numpy(),dtype=torch.float32) ## Computation on top of this array will be executed on GPU\n",
    "    y_data = torch.tensor(y_data.to_numpy(),dtype=torch.float32)\n",
    "    return x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf4ac83-c35b-4523-8915-8695cd785cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1134/3308248062.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_data['Sex'] = x_data['Sex'].apply(lambda x:0 if x=='male' else 1)\n",
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1134/3308248062.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_data['Age'].fillna(x_data['Age'].mean(),inplace=True)\n",
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1134/3308248062.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_data['Fare'].fillna(x_data['Fare'].median(),inplace=True)\n"
     ]
    }
   ],
   "source": [
    "x_data,y_data = preprocess_data('tested.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edfe69a-bd0e-43ea-84b3-606be8befe5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([418, 4]), torch.Size([418]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape,y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ac9d7e-f2e0-4d14-b200-9f5f0c3d8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X,Y,W,b):\n",
    "    Y_pred = X @ W + b\n",
    "    Y_pred = F.sigmoid(Y_pred)\n",
    "    # Y_pred.detach().apply_(lambda x:0 if x<0.5 else 1)\n",
    "    # Y_pred.requires_grad = True\n",
    "    loss = F.binary_cross_entropy(Y_pred,Y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbda3e4e-c6c1-42c9-a33f-c72caf3b12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(W,b,loss):\n",
    "    W.grad = None\n",
    "    b.grad = None\n",
    "    loss.backward()\n",
    "    W.data -= (0.01)*W.grad\n",
    "    b.data -= (0.01)*b.grad\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7edb2283-d98b-40a9-a2e0-3c37f33b28aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y):\n",
    "    W = torch.randn((4,),dtype=torch.float32,requires_grad=True)\n",
    "    b = torch.randn((1,),dtype=torch.float32,requires_grad=True)\n",
    "    for iteration in range(1000):\n",
    "        loss = forward(X,Y,W,b)\n",
    "        W,b = backward(W,b,loss)\n",
    "        print(f\"Iteration {iteration}: loss = {loss}\")\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "77852707-16a1-4e79-9ec6-cbcf5eee087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUmber of epochs = 100\n",
    "# 1 Epoch --> Traversing whole dataset once\n",
    "# How much we are moving W,b towards minimum loss is called learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c35a4ea4-f07c-4f3e-9fde-46708fd05943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 14.177786827087402\n",
      "Iteration 1: loss = 14.026749610900879\n",
      "Iteration 2: loss = 13.930047035217285\n",
      "Iteration 3: loss = 13.850181579589844\n",
      "Iteration 4: loss = 13.693885803222656\n",
      "Iteration 5: loss = 13.618484497070312\n",
      "Iteration 6: loss = 13.545455932617188\n",
      "Iteration 7: loss = 13.44485092163086\n",
      "Iteration 8: loss = 13.369391441345215\n",
      "Iteration 9: loss = 13.26456356048584\n",
      "Iteration 10: loss = 13.186028480529785\n",
      "Iteration 11: loss = 13.10623836517334\n",
      "Iteration 12: loss = 13.024565696716309\n",
      "Iteration 13: loss = 12.941970825195312\n",
      "Iteration 14: loss = 12.857625961303711\n",
      "Iteration 15: loss = 12.716385841369629\n",
      "Iteration 16: loss = 12.59867000579834\n",
      "Iteration 17: loss = 12.505175590515137\n",
      "Iteration 18: loss = 12.410341262817383\n",
      "Iteration 19: loss = 12.315855979919434\n",
      "Iteration 20: loss = 12.220791816711426\n",
      "Iteration 21: loss = 12.124266624450684\n",
      "Iteration 22: loss = 12.025056838989258\n",
      "Iteration 23: loss = 11.92123794555664\n",
      "Iteration 24: loss = 11.818687438964844\n",
      "Iteration 25: loss = 11.68963623046875\n",
      "Iteration 26: loss = 11.383156776428223\n",
      "Iteration 27: loss = 11.248530387878418\n",
      "Iteration 28: loss = 11.11068344116211\n",
      "Iteration 29: loss = 10.996726989746094\n",
      "Iteration 30: loss = 10.880782127380371\n",
      "Iteration 31: loss = 10.765692710876465\n",
      "Iteration 32: loss = 10.648059844970703\n",
      "Iteration 33: loss = 10.52934455871582\n",
      "Iteration 34: loss = 10.409646034240723\n",
      "Iteration 35: loss = 10.28756046295166\n",
      "Iteration 36: loss = 10.162110328674316\n",
      "Iteration 37: loss = 10.003214836120605\n",
      "Iteration 38: loss = 9.871234893798828\n",
      "Iteration 39: loss = 9.70842456817627\n",
      "Iteration 40: loss = 9.566732406616211\n",
      "Iteration 41: loss = 9.420365333557129\n",
      "Iteration 42: loss = 9.26683521270752\n",
      "Iteration 43: loss = 9.110260009765625\n",
      "Iteration 44: loss = 8.953965187072754\n",
      "Iteration 45: loss = 8.798538208007812\n",
      "Iteration 46: loss = 8.612882614135742\n",
      "Iteration 47: loss = 8.4401273727417\n",
      "Iteration 48: loss = 8.199689865112305\n",
      "Iteration 49: loss = 7.9940619468688965\n",
      "Iteration 50: loss = 7.780379772186279\n",
      "Iteration 51: loss = 7.513469696044922\n",
      "Iteration 52: loss = 7.2218546867370605\n",
      "Iteration 53: loss = 6.933192729949951\n",
      "Iteration 54: loss = 6.610634803771973\n",
      "Iteration 55: loss = 6.175811767578125\n",
      "Iteration 56: loss = 5.6567606925964355\n",
      "Iteration 57: loss = 4.983729839324951\n",
      "Iteration 58: loss = 4.309584617614746\n",
      "Iteration 59: loss = 3.614041328430176\n",
      "Iteration 60: loss = 2.9296159744262695\n",
      "Iteration 61: loss = 2.095550298690796\n",
      "Iteration 62: loss = 1.1457881927490234\n",
      "Iteration 63: loss = 0.6882603764533997\n",
      "Iteration 64: loss = 2.4372880458831787\n",
      "Iteration 65: loss = 0.961190402507782\n",
      "Iteration 66: loss = 2.632164239883423\n",
      "Iteration 67: loss = 0.7450383305549622\n",
      "Iteration 68: loss = 2.23419189453125\n",
      "Iteration 69: loss = 2.053572416305542\n",
      "Iteration 70: loss = 2.097348690032959\n",
      "Iteration 71: loss = 2.620849847793579\n",
      "Iteration 72: loss = 1.291513204574585\n",
      "Iteration 73: loss = 4.284661769866943\n",
      "Iteration 74: loss = 0.8793559074401855\n",
      "Iteration 75: loss = 0.6065515279769897\n",
      "Iteration 76: loss = 0.6237947940826416\n",
      "Iteration 77: loss = 3.9834213256835938\n",
      "Iteration 78: loss = 1.0575696229934692\n",
      "Iteration 79: loss = 4.108574867248535\n",
      "Iteration 80: loss = 0.8581117391586304\n",
      "Iteration 81: loss = 0.584487795829773\n",
      "Iteration 82: loss = 0.5537168383598328\n",
      "Iteration 83: loss = 3.185744524002075\n",
      "Iteration 84: loss = 1.354601502418518\n",
      "Iteration 85: loss = 4.231082916259766\n",
      "Iteration 86: loss = 0.8767393827438354\n",
      "Iteration 87: loss = 0.5962929725646973\n",
      "Iteration 88: loss = 0.5475404858589172\n",
      "Iteration 89: loss = 3.13459849357605\n",
      "Iteration 90: loss = 1.364579439163208\n",
      "Iteration 91: loss = 4.231115818023682\n",
      "Iteration 92: loss = 0.8714470863342285\n",
      "Iteration 93: loss = 0.5945262312889099\n",
      "Iteration 94: loss = 0.5707201361656189\n",
      "Iteration 95: loss = 3.4401838779449463\n",
      "Iteration 96: loss = 1.2426244020462036\n",
      "Iteration 97: loss = 4.23996639251709\n",
      "Iteration 98: loss = 0.8843767642974854\n",
      "Iteration 99: loss = 0.5966827273368835\n",
      "Iteration 100: loss = 0.48890116810798645\n",
      "Iteration 101: loss = 1.9035077095031738\n",
      "Iteration 102: loss = 1.8743230104446411\n",
      "Iteration 103: loss = 2.7259888648986816\n",
      "Iteration 104: loss = 1.2284398078918457\n",
      "Iteration 105: loss = 4.255621433258057\n",
      "Iteration 106: loss = 0.8785868287086487\n",
      "Iteration 107: loss = 0.5936207175254822\n",
      "Iteration 108: loss = 0.5097137689590454\n",
      "Iteration 109: loss = 2.7154476642608643\n",
      "Iteration 110: loss = 1.5166690349578857\n",
      "Iteration 111: loss = 4.195099353790283\n",
      "Iteration 112: loss = 0.8429173827171326\n",
      "Iteration 113: loss = 0.6211785674095154\n",
      "Iteration 114: loss = 0.8560511469841003\n",
      "Iteration 115: loss = 4.563996315002441\n",
      "Iteration 116: loss = 0.8933414816856384\n",
      "Iteration 117: loss = 2.3266448974609375\n",
      "Iteration 118: loss = 0.8845181465148926\n",
      "Iteration 119: loss = 4.191713333129883\n",
      "Iteration 120: loss = 0.8387071490287781\n",
      "Iteration 121: loss = 0.637945830821991\n",
      "Iteration 122: loss = 0.9308209419250488\n",
      "Iteration 123: loss = 4.558406829833984\n",
      "Iteration 124: loss = 0.8891724348068237\n",
      "Iteration 125: loss = 2.3132195472717285\n",
      "Iteration 126: loss = 0.8782982230186462\n",
      "Iteration 127: loss = 4.196293354034424\n",
      "Iteration 128: loss = 0.8366485834121704\n",
      "Iteration 129: loss = 0.6385670900344849\n",
      "Iteration 130: loss = 0.9366411566734314\n",
      "Iteration 131: loss = 4.553467273712158\n",
      "Iteration 132: loss = 0.8856781125068665\n",
      "Iteration 133: loss = 2.305468797683716\n",
      "Iteration 134: loss = 0.8738695383071899\n",
      "Iteration 135: loss = 4.197073936462402\n",
      "Iteration 136: loss = 0.834065854549408\n",
      "Iteration 137: loss = 0.6375455856323242\n",
      "Iteration 138: loss = 0.9377412796020508\n",
      "Iteration 139: loss = 4.549713611602783\n",
      "Iteration 140: loss = 0.8823787569999695\n",
      "Iteration 141: loss = 2.2997114658355713\n",
      "Iteration 142: loss = 0.8700584769248962\n",
      "Iteration 143: loss = 4.196679592132568\n",
      "Iteration 144: loss = 0.831340491771698\n",
      "Iteration 145: loss = 0.635726273059845\n",
      "Iteration 146: loss = 0.9371480345726013\n",
      "Iteration 147: loss = 4.546233177185059\n",
      "Iteration 148: loss = 0.8791393637657166\n",
      "Iteration 149: loss = 2.2945032119750977\n",
      "Iteration 150: loss = 0.8664356470108032\n",
      "Iteration 151: loss = 4.195636749267578\n",
      "Iteration 152: loss = 0.8285710215568542\n",
      "Iteration 153: loss = 0.6337414383888245\n",
      "Iteration 154: loss = 0.9360508322715759\n",
      "Iteration 155: loss = 4.542514801025391\n",
      "Iteration 156: loss = 0.875922441482544\n",
      "Iteration 157: loss = 2.2893428802490234\n",
      "Iteration 158: loss = 0.8628649711608887\n",
      "Iteration 159: loss = 4.194761276245117\n",
      "Iteration 160: loss = 0.8257894515991211\n",
      "Iteration 161: loss = 0.631720244884491\n",
      "Iteration 162: loss = 0.9348385334014893\n",
      "Iteration 163: loss = 4.539082050323486\n",
      "Iteration 164: loss = 0.8727166056632996\n",
      "Iteration 165: loss = 2.2843565940856934\n",
      "Iteration 166: loss = 0.8593057990074158\n",
      "Iteration 167: loss = 4.193593502044678\n",
      "Iteration 168: loss = 0.8230065107345581\n",
      "Iteration 169: loss = 0.6297017335891724\n",
      "Iteration 170: loss = 0.9336277842521667\n",
      "Iteration 171: loss = 4.5354084968566895\n",
      "Iteration 172: loss = 0.8695197701454163\n",
      "Iteration 173: loss = 2.279414653778076\n",
      "Iteration 174: loss = 0.8557457327842712\n",
      "Iteration 175: loss = 4.1926045417785645\n",
      "Iteration 176: loss = 0.8202242851257324\n",
      "Iteration 177: loss = 0.6276974678039551\n",
      "Iteration 178: loss = 0.9324568510055542\n",
      "Iteration 179: loss = 4.532067775726318\n",
      "Iteration 180: loss = 0.8663298487663269\n",
      "Iteration 181: loss = 2.274456739425659\n",
      "Iteration 182: loss = 0.8521817326545715\n",
      "Iteration 183: loss = 4.19162130355835\n",
      "Iteration 184: loss = 0.817439079284668\n",
      "Iteration 185: loss = 0.6257131695747375\n",
      "Iteration 186: loss = 0.931335985660553\n",
      "Iteration 187: loss = 4.528397083282471\n",
      "Iteration 188: loss = 0.8631470799446106\n",
      "Iteration 189: loss = 2.0683724880218506\n",
      "Iteration 190: loss = 0.9380927085876465\n",
      "Iteration 191: loss = 4.297298908233643\n",
      "Iteration 192: loss = 0.8317782282829285\n",
      "Iteration 193: loss = 0.5846344828605652\n",
      "Iteration 194: loss = 0.7130734324455261\n",
      "Iteration 195: loss = 4.488381385803223\n",
      "Iteration 196: loss = 0.8668089509010315\n",
      "Iteration 197: loss = 2.3096559047698975\n",
      "Iteration 198: loss = 0.8577554225921631\n",
      "Iteration 199: loss = 4.15468692779541\n",
      "Iteration 200: loss = 0.8075076937675476\n",
      "Iteration 201: loss = 0.6087386012077332\n",
      "Iteration 202: loss = 0.8982041478157043\n",
      "Iteration 203: loss = 4.526595115661621\n",
      "Iteration 204: loss = 0.8583533763885498\n",
      "Iteration 205: loss = 2.2739787101745605\n",
      "Iteration 206: loss = 0.8468570113182068\n",
      "Iteration 207: loss = 4.179531574249268\n",
      "Iteration 208: loss = 0.8081041574478149\n",
      "Iteration 209: loss = 0.6140869855880737\n",
      "Iteration 210: loss = 0.9126425385475159\n",
      "Iteration 211: loss = 4.520469665527344\n",
      "Iteration 212: loss = 0.854038417339325\n",
      "Iteration 213: loss = 2.057664155960083\n",
      "Iteration 214: loss = 0.9290093183517456\n",
      "Iteration 215: loss = 4.291674613952637\n",
      "Iteration 216: loss = 0.8231291174888611\n",
      "Iteration 217: loss = 0.5771897435188293\n",
      "Iteration 218: loss = 0.7059908509254456\n",
      "Iteration 219: loss = 4.477595329284668\n",
      "Iteration 220: loss = 0.8574680685997009\n",
      "Iteration 221: loss = 2.296682596206665\n",
      "Iteration 222: loss = 0.8482211828231812\n",
      "Iteration 223: loss = 4.150876045227051\n",
      "Iteration 224: loss = 0.799244225025177\n",
      "Iteration 225: loss = 0.6017336845397949\n",
      "Iteration 226: loss = 0.8913227319717407\n",
      "Iteration 227: loss = 4.5166778564453125\n",
      "Iteration 228: loss = 0.8489017486572266\n",
      "Iteration 229: loss = 2.0584452152252197\n",
      "Iteration 230: loss = 0.9258509874343872\n",
      "Iteration 231: loss = 4.28392219543457\n",
      "Iteration 232: loss = 0.8174073696136475\n",
      "Iteration 233: loss = 0.5708194971084595\n",
      "Iteration 234: loss = 0.6934292912483215\n",
      "Iteration 235: loss = 4.465453624725342\n",
      "Iteration 236: loss = 0.852511465549469\n",
      "Iteration 237: loss = 2.2966558933258057\n",
      "Iteration 238: loss = 0.8441389799118042\n",
      "Iteration 239: loss = 4.1416544914245605\n",
      "Iteration 240: loss = 0.7928786277770996\n",
      "Iteration 241: loss = 0.594946563243866\n",
      "Iteration 242: loss = 0.8817659020423889\n",
      "Iteration 243: loss = 4.510478496551514\n",
      "Iteration 244: loss = 0.8429567813873291\n",
      "Iteration 245: loss = 2.0516157150268555\n",
      "Iteration 246: loss = 0.9198815226554871\n",
      "Iteration 247: loss = 4.279897212982178\n",
      "Iteration 248: loss = 0.8116970062255859\n",
      "Iteration 249: loss = 0.5658878087997437\n",
      "Iteration 250: loss = 0.6885945200920105\n",
      "Iteration 251: loss = 4.458005428314209\n",
      "Iteration 252: loss = 0.8463602662086487\n",
      "Iteration 253: loss = 2.2884199619293213\n",
      "Iteration 254: loss = 0.8378866910934448\n",
      "Iteration 255: loss = 4.139064788818359\n",
      "Iteration 256: loss = 0.7874031066894531\n",
      "Iteration 257: loss = 0.5902769565582275\n",
      "Iteration 258: loss = 0.877088725566864\n",
      "Iteration 259: loss = 4.5038604736328125\n",
      "Iteration 260: loss = 0.8367218971252441\n",
      "Iteration 261: loss = 1.8410756587982178\n",
      "Iteration 262: loss = 1.0008649826049805\n",
      "Iteration 263: loss = 4.313841819763184\n",
      "Iteration 264: loss = 0.8115178942680359\n",
      "Iteration 265: loss = 0.5582990646362305\n",
      "Iteration 266: loss = 0.6445448398590088\n",
      "Iteration 267: loss = 4.415184020996094\n",
      "Iteration 268: loss = 0.8472152948379517\n",
      "Iteration 269: loss = 2.3289167881011963\n",
      "Iteration 270: loss = 0.8427165746688843\n",
      "Iteration 271: loss = 4.096065998077393\n",
      "Iteration 272: loss = 0.7764096856117249\n",
      "Iteration 273: loss = 0.575477659702301\n",
      "Iteration 274: loss = 0.8522664308547974\n",
      "Iteration 275: loss = 4.4994707107543945\n",
      "Iteration 276: loss = 0.8324621915817261\n",
      "Iteration 277: loss = 2.0491764545440674\n",
      "Iteration 278: loss = 0.911827027797699\n",
      "Iteration 279: loss = 4.266193389892578\n",
      "Iteration 280: loss = 0.8003581166267395\n",
      "Iteration 281: loss = 0.5542392134666443\n",
      "Iteration 282: loss = 0.6683133244514465\n",
      "Iteration 283: loss = 4.435516357421875\n",
      "Iteration 284: loss = 0.835969090461731\n",
      "Iteration 285: loss = 2.28493595123291\n",
      "Iteration 286: loss = 0.8288252949714661\n",
      "Iteration 287: loss = 4.123603343963623\n",
      "Iteration 288: loss = 0.775206446647644\n",
      "Iteration 289: loss = 0.5779428482055664\n",
      "Iteration 290: loss = 0.8604238629341125\n",
      "Iteration 291: loss = 4.4912543296813965\n",
      "Iteration 292: loss = 0.8248226642608643\n",
      "Iteration 293: loss = 1.8263736963272095\n",
      "Iteration 294: loss = 0.9883801937103271\n",
      "Iteration 295: loss = 4.305978298187256\n",
      "Iteration 296: loss = 0.7999779582023621\n",
      "Iteration 297: loss = 0.5489621758460999\n",
      "Iteration 298: loss = 0.6380700469017029\n",
      "Iteration 299: loss = 4.4031662940979\n",
      "Iteration 300: loss = 0.8341729640960693\n",
      "Iteration 301: loss = 2.3070507049560547\n",
      "Iteration 302: loss = 0.8297259211540222\n",
      "Iteration 303: loss = 4.0964274406433105\n",
      "Iteration 304: loss = 0.7664941549301147\n",
      "Iteration 305: loss = 0.5670287013053894\n",
      "Iteration 306: loss = 0.8424184918403625\n",
      "Iteration 307: loss = 4.485760688781738\n",
      "Iteration 308: loss = 0.8198623061180115\n",
      "Iteration 309: loss = 1.8270339965820312\n",
      "Iteration 310: loss = 0.9851357340812683\n",
      "Iteration 311: loss = 4.298770427703857\n",
      "Iteration 312: loss = 0.7945722937583923\n",
      "Iteration 313: loss = 0.5431067943572998\n",
      "Iteration 314: loss = 0.6258397698402405\n",
      "Iteration 315: loss = 4.387484073638916\n",
      "Iteration 316: loss = 0.8300092220306396\n",
      "Iteration 317: loss = 2.3127872943878174\n",
      "Iteration 318: loss = 0.826663613319397\n",
      "Iteration 319: loss = 4.082400798797607\n",
      "Iteration 320: loss = 0.7595254182815552\n",
      "Iteration 321: loss = 0.5598726868629456\n",
      "Iteration 322: loss = 0.8326160311698914\n",
      "Iteration 323: loss = 4.479457378387451\n",
      "Iteration 324: loss = 0.8142952919006348\n",
      "Iteration 325: loss = 1.8223073482513428\n",
      "Iteration 326: loss = 0.979844331741333\n",
      "Iteration 327: loss = 4.293639659881592\n",
      "Iteration 328: loss = 0.7889663577079773\n",
      "Iteration 329: loss = 0.5381410121917725\n",
      "Iteration 330: loss = 0.6197720170021057\n",
      "Iteration 331: loss = 4.3782267570495605\n",
      "Iteration 332: loss = 0.8242952823638916\n",
      "Iteration 333: loss = 2.307476043701172\n",
      "Iteration 334: loss = 0.8213484287261963\n",
      "Iteration 335: loss = 4.077856540679932\n",
      "Iteration 336: loss = 0.7539607286453247\n",
      "Iteration 337: loss = 0.5546779632568359\n",
      "Iteration 338: loss = 0.8260802030563354\n",
      "Iteration 339: loss = 4.472739219665527\n",
      "Iteration 340: loss = 0.808292806148529\n",
      "Iteration 341: loss = 1.8137973546981812\n",
      "Iteration 342: loss = 0.9731353521347046\n",
      "Iteration 343: loss = 4.290389060974121\n",
      "Iteration 344: loss = 0.7832434773445129\n",
      "Iteration 345: loss = 0.5337933897972107\n",
      "Iteration 346: loss = 0.618046224117279\n",
      "Iteration 347: loss = 4.373781204223633\n",
      "Iteration 348: loss = 0.8174493908882141\n",
      "Iteration 349: loss = 2.293790102005005\n",
      "Iteration 350: loss = 0.814342200756073\n",
      "Iteration 351: loss = 4.080565452575684\n",
      "Iteration 352: loss = 0.7494223117828369\n",
      "Iteration 353: loss = 0.5510514378547668\n",
      "Iteration 354: loss = 0.8220179677009583\n",
      "Iteration 355: loss = 4.465790748596191\n",
      "Iteration 356: loss = 0.8019797801971436\n",
      "Iteration 357: loss = 1.8026187419891357\n",
      "Iteration 358: loss = 0.9653280377388\n",
      "Iteration 359: loss = 4.2883076667785645\n",
      "Iteration 360: loss = 0.7774147987365723\n",
      "Iteration 361: loss = 0.5299285650253296\n",
      "Iteration 362: loss = 0.619553804397583\n",
      "Iteration 363: loss = 4.372753620147705\n",
      "Iteration 364: loss = 0.8098897933959961\n",
      "Iteration 365: loss = 2.2746033668518066\n",
      "Iteration 366: loss = 0.8059701323509216\n",
      "Iteration 367: loss = 4.087244033813477\n",
      "Iteration 368: loss = 0.7454103827476501\n",
      "Iteration 369: loss = 0.548678457736969\n",
      "Iteration 370: loss = 0.820909321308136\n",
      "Iteration 371: loss = 4.458784103393555\n",
      "Iteration 372: loss = 0.7954896688461304\n",
      "Iteration 373: loss = 1.789832592010498\n",
      "Iteration 374: loss = 0.9567456245422363\n",
      "Iteration 375: loss = 4.286892890930176\n",
      "Iteration 376: loss = 0.7715318202972412\n",
      "Iteration 377: loss = 0.526404857635498\n",
      "Iteration 378: loss = 0.6231454610824585\n",
      "Iteration 379: loss = 4.373408317565918\n",
      "Iteration 380: loss = 0.8020568490028381\n",
      "Iteration 381: loss = 2.2532312870025635\n",
      "Iteration 382: loss = 0.7966533303260803\n",
      "Iteration 383: loss = 4.09494686126709\n",
      "Iteration 384: loss = 0.7414630651473999\n",
      "Iteration 385: loss = 0.5471444129943848\n",
      "Iteration 386: loss = 0.8228651881217957\n",
      "Iteration 387: loss = 4.451786518096924\n",
      "Iteration 388: loss = 0.7889572381973267\n",
      "Iteration 389: loss = 1.5737628936767578\n",
      "Iteration 390: loss = 1.0345879793167114\n",
      "Iteration 391: loss = 4.28645658493042\n",
      "Iteration 392: loss = 0.7636771202087402\n",
      "Iteration 393: loss = 0.5282313227653503\n",
      "Iteration 394: loss = 0.6637693643569946\n",
      "Iteration 395: loss = 4.401736259460449\n",
      "Iteration 396: loss = 0.7890729904174805\n",
      "Iteration 397: loss = 1.7909871339797974\n",
      "Iteration 398: loss = 0.9508752226829529\n",
      "Iteration 399: loss = 4.275450229644775\n",
      "Iteration 400: loss = 0.7636147141456604\n",
      "Iteration 401: loss = 0.5177652835845947\n",
      "Iteration 402: loss = 0.6042152643203735\n",
      "Iteration 403: loss = 4.349100112915039\n",
      "Iteration 404: loss = 0.7958521246910095\n",
      "Iteration 405: loss = 2.2621843814849854\n",
      "Iteration 406: loss = 0.7929750084877014\n",
      "Iteration 407: loss = 4.075527191162109\n",
      "Iteration 408: loss = 0.7315929532051086\n",
      "Iteration 409: loss = 0.5360071659088135\n",
      "Iteration 410: loss = 0.8046692609786987\n",
      "Iteration 411: loss = 4.441890239715576\n",
      "Iteration 412: loss = 0.7807790040969849\n",
      "Iteration 413: loss = 1.5668528079986572\n",
      "Iteration 414: loss = 1.0266482830047607\n",
      "Iteration 415: loss = 4.279326438903809\n",
      "Iteration 416: loss = 0.755448043346405\n",
      "Iteration 417: loss = 0.5209078192710876\n",
      "Iteration 418: loss = 0.6545578837394714\n",
      "Iteration 419: loss = 4.389074802398682\n",
      "Iteration 420: loss = 0.7805293202400208\n",
      "Iteration 421: loss = 1.7816106081008911\n",
      "Iteration 422: loss = 0.9421116709709167\n",
      "Iteration 423: loss = 4.269180774688721\n",
      "Iteration 424: loss = 0.7552977800369263\n",
      "Iteration 425: loss = 0.5110349655151367\n",
      "Iteration 426: loss = 0.5983601808547974\n",
      "Iteration 427: loss = 4.338419437408447\n",
      "Iteration 428: loss = 0.7866976857185364\n",
      "Iteration 429: loss = 2.2489664554595947\n",
      "Iteration 430: loss = 0.78383469581604\n",
      "Iteration 431: loss = 4.072981357574463\n",
      "Iteration 432: loss = 0.7239388823509216\n",
      "Iteration 433: loss = 0.5297213196754456\n",
      "Iteration 434: loss = 0.7977996468544006\n",
      "Iteration 435: loss = 4.431698322296143\n",
      "Iteration 436: loss = 0.771811842918396\n",
      "Iteration 437: loss = 1.551477313041687\n",
      "Iteration 438: loss = 1.0155802965164185\n",
      "Iteration 439: loss = 4.275115966796875\n",
      "Iteration 440: loss = 0.747033953666687\n",
      "Iteration 441: loss = 0.5152612328529358\n",
      "Iteration 442: loss = 0.6544481515884399\n",
      "Iteration 443: loss = 4.382124900817871\n",
      "Iteration 444: loss = 0.7709876298904419\n",
      "Iteration 445: loss = 1.5615813732147217\n",
      "Iteration 446: loss = 1.0169137716293335\n",
      "Iteration 447: loss = 4.268067836761475\n",
      "Iteration 448: loss = 0.7447642683982849\n",
      "Iteration 449: loss = 0.510351300239563\n",
      "Iteration 450: loss = 0.6364415287971497\n",
      "Iteration 451: loss = 4.367898941040039\n",
      "Iteration 452: loss = 0.7699637413024902\n",
      "Iteration 453: loss = 1.7751222848892212\n",
      "Iteration 454: loss = 0.93280029296875\n",
      "Iteration 455: loss = 4.258288860321045\n",
      "Iteration 456: loss = 0.7445610165596008\n",
      "Iteration 457: loss = 0.5012826323509216\n",
      "Iteration 458: loss = 0.5837656259536743\n",
      "Iteration 459: loss = 3.9138271808624268\n",
      "Iteration 460: loss = 0.8462340235710144\n",
      "Iteration 461: loss = 2.9432828426361084\n",
      "Iteration 462: loss = 0.7060231566429138\n",
      "Iteration 463: loss = 0.9924185872077942\n",
      "Iteration 464: loss = 1.2077680826187134\n",
      "Iteration 465: loss = 4.26875638961792\n",
      "Iteration 466: loss = 0.737185537815094\n",
      "Iteration 467: loss = 0.9003354907035828\n",
      "Iteration 468: loss = 1.0357807874679565\n",
      "Iteration 469: loss = 4.337955951690674\n",
      "Iteration 470: loss = 0.7383758425712585\n",
      "Iteration 471: loss = 0.8928342461585999\n",
      "Iteration 472: loss = 1.0198674201965332\n",
      "Iteration 473: loss = 4.341335773468018\n",
      "Iteration 474: loss = 0.7370284199714661\n",
      "Iteration 475: loss = 0.8902232050895691\n",
      "Iteration 476: loss = 1.016301155090332\n",
      "Iteration 477: loss = 4.340792179107666\n",
      "Iteration 478: loss = 0.7356685400009155\n",
      "Iteration 479: loss = 0.8892362117767334\n",
      "Iteration 480: loss = 1.0152078866958618\n",
      "Iteration 481: loss = 4.339319229125977\n",
      "Iteration 482: loss = 0.7342331409454346\n",
      "Iteration 483: loss = 0.8873674869537354\n",
      "Iteration 484: loss = 1.0130152702331543\n",
      "Iteration 485: loss = 4.338290214538574\n",
      "Iteration 486: loss = 0.7328468561172485\n",
      "Iteration 487: loss = 0.8861446380615234\n",
      "Iteration 488: loss = 1.0115833282470703\n",
      "Iteration 489: loss = 4.336928844451904\n",
      "Iteration 490: loss = 0.7314307689666748\n",
      "Iteration 491: loss = 0.8846114873886108\n",
      "Iteration 492: loss = 1.009647250175476\n",
      "Iteration 493: loss = 4.335753917694092\n",
      "Iteration 494: loss = 0.7300373315811157\n",
      "Iteration 495: loss = 0.883171021938324\n",
      "Iteration 496: loss = 1.0080461502075195\n",
      "Iteration 497: loss = 4.3345232009887695\n",
      "Iteration 498: loss = 0.7286322116851807\n",
      "Iteration 499: loss = 0.8816544413566589\n",
      "Iteration 500: loss = 1.0062239170074463\n",
      "Iteration 501: loss = 4.333326816558838\n",
      "Iteration 502: loss = 0.727238118648529\n",
      "Iteration 503: loss = 0.8802974224090576\n",
      "Iteration 504: loss = 1.0045514106750488\n",
      "Iteration 505: loss = 4.33203649520874\n",
      "Iteration 506: loss = 0.7258394956588745\n",
      "Iteration 507: loss = 0.8787490725517273\n",
      "Iteration 508: loss = 1.0027787685394287\n",
      "Iteration 509: loss = 4.330835819244385\n",
      "Iteration 510: loss = 0.7244469523429871\n",
      "Iteration 511: loss = 0.8773717284202576\n",
      "Iteration 512: loss = 1.0010749101638794\n",
      "Iteration 513: loss = 4.329590320587158\n",
      "Iteration 514: loss = 0.7230538725852966\n",
      "Iteration 515: loss = 0.8758096098899841\n",
      "Iteration 516: loss = 0.9993253350257874\n",
      "Iteration 517: loss = 4.328361988067627\n",
      "Iteration 518: loss = 0.7216641306877136\n",
      "Iteration 519: loss = 0.8744421601295471\n",
      "Iteration 520: loss = 0.9976067543029785\n",
      "Iteration 521: loss = 4.327091693878174\n",
      "Iteration 522: loss = 0.7202748656272888\n",
      "Iteration 523: loss = 0.8729026317596436\n",
      "Iteration 524: loss = 0.9958693385124207\n",
      "Iteration 525: loss = 4.325924396514893\n",
      "Iteration 526: loss = 0.7188883423805237\n",
      "Iteration 527: loss = 0.871529221534729\n",
      "Iteration 528: loss = 0.9941474199295044\n",
      "Iteration 529: loss = 4.324673652648926\n",
      "Iteration 530: loss = 0.7175038456916809\n",
      "Iteration 531: loss = 0.8700366020202637\n",
      "Iteration 532: loss = 0.9924166798591614\n",
      "Iteration 533: loss = 4.3234052658081055\n",
      "Iteration 534: loss = 0.7161213159561157\n",
      "Iteration 535: loss = 0.8686405420303345\n",
      "Iteration 536: loss = 0.990689754486084\n",
      "Iteration 537: loss = 4.322131156921387\n",
      "Iteration 538: loss = 0.7147398591041565\n",
      "Iteration 539: loss = 0.8671605587005615\n",
      "Iteration 540: loss = 0.9889643788337708\n",
      "Iteration 541: loss = 4.320883750915527\n",
      "Iteration 542: loss = 0.7133609652519226\n",
      "Iteration 543: loss = 0.8657518625259399\n",
      "Iteration 544: loss = 0.9872393012046814\n",
      "Iteration 545: loss = 4.319641590118408\n",
      "Iteration 546: loss = 0.7119837403297424\n",
      "Iteration 547: loss = 0.8643078207969666\n",
      "Iteration 548: loss = 0.9855141043663025\n",
      "Iteration 549: loss = 4.31839656829834\n",
      "Iteration 550: loss = 0.7106083631515503\n",
      "Iteration 551: loss = 0.8628981113433838\n",
      "Iteration 552: loss = 0.983791172504425\n",
      "Iteration 553: loss = 4.317190170288086\n",
      "Iteration 554: loss = 0.7092347741127014\n",
      "Iteration 555: loss = 0.8614450097084045\n",
      "Iteration 556: loss = 0.9820683598518372\n",
      "Iteration 557: loss = 4.315945625305176\n",
      "Iteration 558: loss = 0.7078632712364197\n",
      "Iteration 559: loss = 0.8600508570671082\n",
      "Iteration 560: loss = 0.980347216129303\n",
      "Iteration 561: loss = 4.314614295959473\n",
      "Iteration 562: loss = 0.7064935564994812\n",
      "Iteration 563: loss = 0.8586260676383972\n",
      "Iteration 564: loss = 0.9786261320114136\n",
      "Iteration 565: loss = 4.313378810882568\n",
      "Iteration 566: loss = 0.705125629901886\n",
      "Iteration 567: loss = 0.8572388887405396\n",
      "Iteration 568: loss = 0.9769065976142883\n",
      "Iteration 569: loss = 4.312127113342285\n",
      "Iteration 570: loss = 0.7037599682807922\n",
      "Iteration 571: loss = 0.8557758331298828\n",
      "Iteration 572: loss = 0.9751881957054138\n",
      "Iteration 573: loss = 4.310843467712402\n",
      "Iteration 574: loss = 0.7023957371711731\n",
      "Iteration 575: loss = 0.8543460965156555\n",
      "Iteration 576: loss = 0.9734702110290527\n",
      "Iteration 577: loss = 4.3095927238464355\n",
      "Iteration 578: loss = 0.7010337710380554\n",
      "Iteration 579: loss = 0.8529708981513977\n",
      "Iteration 580: loss = 0.9717528223991394\n",
      "Iteration 581: loss = 4.308372497558594\n",
      "Iteration 582: loss = 0.6996732354164124\n",
      "Iteration 583: loss = 0.8515303730964661\n",
      "Iteration 584: loss = 0.9700363874435425\n",
      "Iteration 585: loss = 4.307031631469727\n",
      "Iteration 586: loss = 0.6983149647712708\n",
      "Iteration 587: loss = 0.8501846790313721\n",
      "Iteration 588: loss = 0.968323290348053\n",
      "Iteration 589: loss = 4.305781841278076\n",
      "Iteration 590: loss = 0.6969584822654724\n",
      "Iteration 591: loss = 0.8487496972084045\n",
      "Iteration 592: loss = 0.9666086435317993\n",
      "Iteration 593: loss = 4.304527282714844\n",
      "Iteration 594: loss = 0.6956037282943726\n",
      "Iteration 595: loss = 0.646246612071991\n",
      "Iteration 596: loss = 1.0683448314666748\n",
      "Iteration 597: loss = 4.286056995391846\n",
      "Iteration 598: loss = 0.7001615166664124\n",
      "Iteration 599: loss = 0.9364316463470459\n",
      "Iteration 600: loss = 1.0701286792755127\n",
      "Iteration 601: loss = 4.2528815269470215\n",
      "Iteration 602: loss = 0.6882686614990234\n",
      "Iteration 603: loss = 0.5911086797714233\n",
      "Iteration 604: loss = 0.9810053110122681\n",
      "Iteration 605: loss = 4.317697048187256\n",
      "Iteration 606: loss = 0.701063334941864\n",
      "Iteration 607: loss = 0.9645761847496033\n",
      "Iteration 608: loss = 1.0928544998168945\n",
      "Iteration 609: loss = 4.234834671020508\n",
      "Iteration 610: loss = 0.6840590238571167\n",
      "Iteration 611: loss = 0.5684751272201538\n",
      "Iteration 612: loss = 0.9402569532394409\n",
      "Iteration 613: loss = 4.326776504516602\n",
      "Iteration 614: loss = 0.6994459629058838\n",
      "Iteration 615: loss = 0.9733201265335083\n",
      "Iteration 616: loss = 1.0973308086395264\n",
      "Iteration 617: loss = 4.227768898010254\n",
      "Iteration 618: loss = 0.680947482585907\n",
      "Iteration 619: loss = 0.5600760579109192\n",
      "Iteration 620: loss = 0.9251745343208313\n",
      "Iteration 621: loss = 4.327137470245361\n",
      "Iteration 622: loss = 0.6970012784004211\n",
      "Iteration 623: loss = 0.9728600382804871\n",
      "Iteration 624: loss = 1.0959407091140747\n",
      "Iteration 625: loss = 4.224430561065674\n",
      "Iteration 626: loss = 0.6781892776489258\n",
      "Iteration 627: loss = 0.5565312504768372\n",
      "Iteration 628: loss = 0.9194848537445068\n",
      "Iteration 629: loss = 4.324837684631348\n",
      "Iteration 630: loss = 0.6943027377128601\n",
      "Iteration 631: loss = 0.9693959951400757\n",
      "Iteration 632: loss = 1.0926905870437622\n",
      "Iteration 633: loss = 4.222276210784912\n",
      "Iteration 634: loss = 0.6755461096763611\n",
      "Iteration 635: loss = 0.554344117641449\n",
      "Iteration 636: loss = 0.9168351292610168\n",
      "Iteration 637: loss = 4.321763515472412\n",
      "Iteration 638: loss = 0.691530704498291\n",
      "Iteration 639: loss = 0.9659499526023865\n",
      "Iteration 640: loss = 1.0888595581054688\n",
      "Iteration 641: loss = 4.220540523529053\n",
      "Iteration 642: loss = 0.6729414463043213\n",
      "Iteration 643: loss = 0.5525836944580078\n",
      "Iteration 644: loss = 0.915115237236023\n",
      "Iteration 645: loss = 4.318390846252441\n",
      "Iteration 646: loss = 0.6887422204017639\n",
      "Iteration 647: loss = 0.9620162844657898\n",
      "Iteration 648: loss = 1.0848506689071655\n",
      "Iteration 649: loss = 4.218868732452393\n",
      "Iteration 650: loss = 0.6703542470932007\n",
      "Iteration 651: loss = 0.550943911075592\n",
      "Iteration 652: loss = 0.9136593341827393\n",
      "Iteration 653: loss = 4.315029621124268\n",
      "Iteration 654: loss = 0.6859540343284607\n",
      "Iteration 655: loss = 0.9561033844947815\n",
      "Iteration 656: loss = 1.080788254737854\n",
      "Iteration 657: loss = 4.217226505279541\n",
      "Iteration 658: loss = 0.6677764654159546\n",
      "Iteration 659: loss = 0.5489009618759155\n",
      "Iteration 660: loss = 0.9122628569602966\n",
      "Iteration 661: loss = 4.31160831451416\n",
      "Iteration 662: loss = 0.6831720471382141\n",
      "Iteration 663: loss = 0.952383816242218\n",
      "Iteration 664: loss = 1.0767152309417725\n",
      "Iteration 665: loss = 4.215573310852051\n",
      "Iteration 666: loss = 0.665205717086792\n",
      "Iteration 667: loss = 0.5473070740699768\n",
      "Iteration 668: loss = 0.9108601808547974\n",
      "Iteration 669: loss = 4.308199405670166\n",
      "Iteration 670: loss = 0.6803982853889465\n",
      "Iteration 671: loss = 0.9482058882713318\n",
      "Iteration 672: loss = 1.0726401805877686\n",
      "Iteration 673: loss = 4.213908672332764\n",
      "Iteration 674: loss = 0.6626417636871338\n",
      "Iteration 675: loss = 0.545706033706665\n",
      "Iteration 676: loss = 0.909436047077179\n",
      "Iteration 677: loss = 4.304784297943115\n",
      "Iteration 678: loss = 0.6776324510574341\n",
      "Iteration 679: loss = 0.9443989396095276\n",
      "Iteration 680: loss = 1.0685704946517944\n",
      "Iteration 681: loss = 4.212198734283447\n",
      "Iteration 682: loss = 0.6600835919380188\n",
      "Iteration 683: loss = 0.5440974235534668\n",
      "Iteration 684: loss = 0.9079830646514893\n",
      "Iteration 685: loss = 4.301380634307861\n",
      "Iteration 686: loss = 0.6748754382133484\n",
      "Iteration 687: loss = 0.9405075311660767\n",
      "Iteration 688: loss = 1.064505934715271\n",
      "Iteration 689: loss = 4.210535049438477\n",
      "Iteration 690: loss = 0.6575323343276978\n",
      "Iteration 691: loss = 0.542487382888794\n",
      "Iteration 692: loss = 0.9064996242523193\n",
      "Iteration 693: loss = 4.2980194091796875\n",
      "Iteration 694: loss = 0.6721271872520447\n",
      "Iteration 695: loss = 0.9367519617080688\n",
      "Iteration 696: loss = 1.0604469776153564\n",
      "Iteration 697: loss = 4.208791732788086\n",
      "Iteration 698: loss = 0.6549873948097229\n",
      "Iteration 699: loss = 0.5408728122711182\n",
      "Iteration 700: loss = 0.9049851894378662\n",
      "Iteration 701: loss = 4.29463005065918\n",
      "Iteration 702: loss = 0.6693874597549438\n",
      "Iteration 703: loss = 0.9327343106269836\n",
      "Iteration 704: loss = 1.0563950538635254\n",
      "Iteration 705: loss = 4.207078456878662\n",
      "Iteration 706: loss = 0.6524485349655151\n",
      "Iteration 707: loss = 0.5392463207244873\n",
      "Iteration 708: loss = 0.9034400582313538\n",
      "Iteration 709: loss = 4.291253566741943\n",
      "Iteration 710: loss = 0.6666565537452698\n",
      "Iteration 711: loss = 0.9288707971572876\n",
      "Iteration 712: loss = 1.0523507595062256\n",
      "Iteration 713: loss = 4.205308437347412\n",
      "Iteration 714: loss = 0.6499157547950745\n",
      "Iteration 715: loss = 0.5376180410385132\n",
      "Iteration 716: loss = 0.9018639922142029\n",
      "Iteration 717: loss = 4.28787899017334\n",
      "Iteration 718: loss = 0.663934051990509\n",
      "Iteration 719: loss = 0.9242085218429565\n",
      "Iteration 720: loss = 1.0483105182647705\n",
      "Iteration 721: loss = 4.203573226928711\n",
      "Iteration 722: loss = 0.6473898887634277\n",
      "Iteration 723: loss = 0.5359763503074646\n",
      "Iteration 724: loss = 0.9002630114555359\n",
      "Iteration 725: loss = 4.284547805786133\n",
      "Iteration 726: loss = 0.6612205505371094\n",
      "Iteration 727: loss = 0.9203760027885437\n",
      "Iteration 728: loss = 1.0442770719528198\n",
      "Iteration 729: loss = 4.20180606842041\n",
      "Iteration 730: loss = 0.6448702812194824\n",
      "Iteration 731: loss = 0.5343396663665771\n",
      "Iteration 732: loss = 0.8986316323280334\n",
      "Iteration 733: loss = 4.281209468841553\n",
      "Iteration 734: loss = 0.6585155129432678\n",
      "Iteration 735: loss = 0.916602373123169\n",
      "Iteration 736: loss = 1.0402511358261108\n",
      "Iteration 737: loss = 4.199997425079346\n",
      "Iteration 738: loss = 0.6423574686050415\n",
      "Iteration 739: loss = 0.532689094543457\n",
      "Iteration 740: loss = 0.8969733119010925\n",
      "Iteration 741: loss = 4.27790641784668\n",
      "Iteration 742: loss = 0.655819296836853\n",
      "Iteration 743: loss = 0.9128592014312744\n",
      "Iteration 744: loss = 1.0362311601638794\n",
      "Iteration 745: loss = 4.19822359085083\n",
      "Iteration 746: loss = 0.6398506760597229\n",
      "Iteration 747: loss = 0.5310392379760742\n",
      "Iteration 748: loss = 0.8952876329421997\n",
      "Iteration 749: loss = 4.27457332611084\n",
      "Iteration 750: loss = 0.6531314849853516\n",
      "Iteration 751: loss = 0.9091708660125732\n",
      "Iteration 752: loss = 1.0322182178497314\n",
      "Iteration 753: loss = 4.196357727050781\n",
      "Iteration 754: loss = 0.63735032081604\n",
      "Iteration 755: loss = 0.5293835401535034\n",
      "Iteration 756: loss = 0.893576979637146\n",
      "Iteration 757: loss = 4.2712273597717285\n",
      "Iteration 758: loss = 0.6504526138305664\n",
      "Iteration 759: loss = 0.9049549698829651\n",
      "Iteration 760: loss = 1.0282127857208252\n",
      "Iteration 761: loss = 4.194536209106445\n",
      "Iteration 762: loss = 0.6348568797111511\n",
      "Iteration 763: loss = 0.527718186378479\n",
      "Iteration 764: loss = 0.8918384313583374\n",
      "Iteration 765: loss = 4.26793098449707\n",
      "Iteration 766: loss = 0.6477821469306946\n",
      "Iteration 767: loss = 0.9013029336929321\n",
      "Iteration 768: loss = 1.0242139101028442\n",
      "Iteration 769: loss = 4.1926493644714355\n",
      "Iteration 770: loss = 0.6323697566986084\n",
      "Iteration 771: loss = 0.5256810784339905\n",
      "Iteration 772: loss = 0.8900744915008545\n",
      "Iteration 773: loss = 4.264636516571045\n",
      "Iteration 774: loss = 0.6451201438903809\n",
      "Iteration 775: loss = 0.8976443409919739\n",
      "Iteration 776: loss = 1.0202221870422363\n",
      "Iteration 777: loss = 4.1907958984375\n",
      "Iteration 778: loss = 0.6298889517784119\n",
      "Iteration 779: loss = 0.5240095853805542\n",
      "Iteration 780: loss = 0.8882871270179749\n",
      "Iteration 781: loss = 4.2613725662231445\n",
      "Iteration 782: loss = 0.6424673795700073\n",
      "Iteration 783: loss = 0.8940960168838501\n",
      "Iteration 784: loss = 1.016237735748291\n",
      "Iteration 785: loss = 4.188918113708496\n",
      "Iteration 786: loss = 0.627415120601654\n",
      "Iteration 787: loss = 0.5223309397697449\n",
      "Iteration 788: loss = 0.8864742517471313\n",
      "Iteration 789: loss = 4.258052349090576\n",
      "Iteration 790: loss = 0.6398228406906128\n",
      "Iteration 791: loss = 0.8898921608924866\n",
      "Iteration 792: loss = 1.0122594833374023\n",
      "Iteration 793: loss = 4.187005996704102\n",
      "Iteration 794: loss = 0.624947726726532\n",
      "Iteration 795: loss = 0.5206487774848938\n",
      "Iteration 796: loss = 0.8846393823623657\n",
      "Iteration 797: loss = 4.254773139953613\n",
      "Iteration 798: loss = 0.6371867060661316\n",
      "Iteration 799: loss = 0.8863425850868225\n",
      "Iteration 800: loss = 1.0082879066467285\n",
      "Iteration 801: loss = 4.185105323791504\n",
      "Iteration 802: loss = 0.6224871277809143\n",
      "Iteration 803: loss = 0.5189623832702637\n",
      "Iteration 804: loss = 0.8827813863754272\n",
      "Iteration 805: loss = 4.251542568206787\n",
      "Iteration 806: loss = 0.6345594525337219\n",
      "Iteration 807: loss = 0.8828916549682617\n",
      "Iteration 808: loss = 1.0043258666992188\n",
      "Iteration 809: loss = 4.183155536651611\n",
      "Iteration 810: loss = 0.6200329065322876\n",
      "Iteration 811: loss = 0.5172727108001709\n",
      "Iteration 812: loss = 0.8809000849723816\n",
      "Iteration 813: loss = 4.248254299163818\n",
      "Iteration 814: loss = 0.6319405436515808\n",
      "Iteration 815: loss = 0.8789578080177307\n",
      "Iteration 816: loss = 1.0003679990768433\n",
      "Iteration 817: loss = 4.181212425231934\n",
      "Iteration 818: loss = 0.6175855398178101\n",
      "Iteration 819: loss = 0.5155776739120483\n",
      "Iteration 820: loss = 0.8789986968040466\n",
      "Iteration 821: loss = 4.245035648345947\n",
      "Iteration 822: loss = 0.6293305158615112\n",
      "Iteration 823: loss = 0.8754807114601135\n",
      "Iteration 824: loss = 0.9964183568954468\n",
      "Iteration 825: loss = 4.1792426109313965\n",
      "Iteration 826: loss = 0.6151448488235474\n",
      "Iteration 827: loss = 0.5138798952102661\n",
      "Iteration 828: loss = 0.8770744800567627\n",
      "Iteration 829: loss = 4.241757392883301\n",
      "Iteration 830: loss = 0.6267287731170654\n",
      "Iteration 831: loss = 0.8720625042915344\n",
      "Iteration 832: loss = 0.992476761341095\n",
      "Iteration 833: loss = 4.17728853225708\n",
      "Iteration 834: loss = 0.6127108335494995\n",
      "Iteration 835: loss = 0.5118631720542908\n",
      "Iteration 836: loss = 0.8751286864280701\n",
      "Iteration 837: loss = 4.238523483276367\n",
      "Iteration 838: loss = 0.6241357922554016\n",
      "Iteration 839: loss = 0.8682221174240112\n",
      "Iteration 840: loss = 0.9885419011116028\n",
      "Iteration 841: loss = 4.175281524658203\n",
      "Iteration 842: loss = 0.6102834939956665\n",
      "Iteration 843: loss = 0.5101577639579773\n",
      "Iteration 844: loss = 0.8731634616851807\n",
      "Iteration 845: loss = 4.235307216644287\n",
      "Iteration 846: loss = 0.6215510964393616\n",
      "Iteration 847: loss = 0.8648689389228821\n",
      "Iteration 848: loss = 0.9846132397651672\n",
      "Iteration 849: loss = 4.173301696777344\n",
      "Iteration 850: loss = 0.6078631281852722\n",
      "Iteration 851: loss = 0.5084469318389893\n",
      "Iteration 852: loss = 0.8711798191070557\n",
      "Iteration 853: loss = 4.232056617736816\n",
      "Iteration 854: loss = 0.6189749240875244\n",
      "Iteration 855: loss = 0.8611427545547485\n",
      "Iteration 856: loss = 0.9806923866271973\n",
      "Iteration 857: loss = 4.171261787414551\n",
      "Iteration 858: loss = 0.6054495573043823\n",
      "Iteration 859: loss = 0.506737232208252\n",
      "Iteration 860: loss = 0.8691760897636414\n",
      "Iteration 861: loss = 4.228854179382324\n",
      "Iteration 862: loss = 0.6164073944091797\n",
      "Iteration 863: loss = 0.8578339219093323\n",
      "Iteration 864: loss = 0.9767777919769287\n",
      "Iteration 865: loss = 4.169220924377441\n",
      "Iteration 866: loss = 0.6030424237251282\n",
      "Iteration 867: loss = 0.5050233602523804\n",
      "Iteration 868: loss = 0.8671534657478333\n",
      "Iteration 869: loss = 4.225600719451904\n",
      "Iteration 870: loss = 0.6138481497764587\n",
      "Iteration 871: loss = 0.8542136549949646\n",
      "Iteration 872: loss = 0.9728710651397705\n",
      "Iteration 873: loss = 4.167177200317383\n",
      "Iteration 874: loss = 0.6006424427032471\n",
      "Iteration 875: loss = 0.503303587436676\n",
      "Iteration 876: loss = 0.865111768245697\n",
      "Iteration 877: loss = 4.222430229187012\n",
      "Iteration 878: loss = 0.6112976670265198\n",
      "Iteration 879: loss = 0.8509038090705872\n",
      "Iteration 880: loss = 0.9689723253250122\n",
      "Iteration 881: loss = 4.165111064910889\n",
      "Iteration 882: loss = 0.5982489585876465\n",
      "Iteration 883: loss = 0.5013037323951721\n",
      "Iteration 884: loss = 0.8630542159080505\n",
      "Iteration 885: loss = 4.219202518463135\n",
      "Iteration 886: loss = 0.608755350112915\n",
      "Iteration 887: loss = 0.8473893404006958\n",
      "Iteration 888: loss = 0.9650797247886658\n",
      "Iteration 889: loss = 4.163057327270508\n",
      "Iteration 890: loss = 0.5958624482154846\n",
      "Iteration 891: loss = 0.49958065152168274\n",
      "Iteration 892: loss = 0.8609777092933655\n",
      "Iteration 893: loss = 4.216012001037598\n",
      "Iteration 894: loss = 0.6062214970588684\n",
      "Iteration 895: loss = 0.8441360592842102\n",
      "Iteration 896: loss = 0.9611936211585999\n",
      "Iteration 897: loss = 4.160964012145996\n",
      "Iteration 898: loss = 0.5934827923774719\n",
      "Iteration 899: loss = 0.49785712361335754\n",
      "Iteration 900: loss = 0.8588864207267761\n",
      "Iteration 901: loss = 4.212827682495117\n",
      "Iteration 902: loss = 0.6036964654922485\n",
      "Iteration 903: loss = 0.8405970335006714\n",
      "Iteration 904: loss = 0.9573155641555786\n",
      "Iteration 905: loss = 4.158878326416016\n",
      "Iteration 906: loss = 0.5911096334457397\n",
      "Iteration 907: loss = 0.496130108833313\n",
      "Iteration 908: loss = 0.8567770719528198\n",
      "Iteration 909: loss = 4.209631443023682\n",
      "Iteration 910: loss = 0.601179301738739\n",
      "Iteration 911: loss = 0.837168276309967\n",
      "Iteration 912: loss = 0.9534428715705872\n",
      "Iteration 913: loss = 4.156757831573486\n",
      "Iteration 914: loss = 0.5887439250946045\n",
      "Iteration 915: loss = 0.4944014251232147\n",
      "Iteration 916: loss = 0.8546527624130249\n",
      "Iteration 917: loss = 4.206459045410156\n",
      "Iteration 918: loss = 0.5986706018447876\n",
      "Iteration 919: loss = 0.833817183971405\n",
      "Iteration 920: loss = 0.9495783448219299\n",
      "Iteration 921: loss = 4.154644012451172\n",
      "Iteration 922: loss = 0.5863847732543945\n",
      "Iteration 923: loss = 0.4924207627773285\n",
      "Iteration 924: loss = 0.8525134325027466\n",
      "Iteration 925: loss = 4.2032694816589355\n",
      "Iteration 926: loss = 0.5961706042289734\n",
      "Iteration 927: loss = 0.830600917339325\n",
      "Iteration 928: loss = 0.9457211494445801\n",
      "Iteration 929: loss = 4.152499198913574\n",
      "Iteration 930: loss = 0.5840325355529785\n",
      "Iteration 931: loss = 0.4906906187534332\n",
      "Iteration 932: loss = 0.8503583073616028\n",
      "Iteration 933: loss = 3.9990265369415283\n",
      "Iteration 934: loss = 0.6170449256896973\n",
      "Iteration 935: loss = 0.9315233826637268\n",
      "Iteration 936: loss = 1.0038255453109741\n",
      "Iteration 937: loss = 4.112579822540283\n",
      "Iteration 938: loss = 0.5790637731552124\n",
      "Iteration 939: loss = 0.4310533404350281\n",
      "Iteration 940: loss = 0.7081291079521179\n",
      "Iteration 941: loss = 4.007580757141113\n",
      "Iteration 942: loss = 0.6188048720359802\n",
      "Iteration 943: loss = 1.1617039442062378\n",
      "Iteration 944: loss = 0.9051346778869629\n",
      "Iteration 945: loss = 4.121286869049072\n",
      "Iteration 946: loss = 0.581160306930542\n",
      "Iteration 947: loss = 0.3965629041194916\n",
      "Iteration 948: loss = 0.5721544027328491\n",
      "Iteration 949: loss = 3.7664105892181396\n",
      "Iteration 950: loss = 0.6442111134529114\n",
      "Iteration 951: loss = 1.7225406169891357\n",
      "Iteration 952: loss = 0.7659421563148499\n",
      "Iteration 953: loss = 4.052902698516846\n",
      "Iteration 954: loss = 0.5892755389213562\n",
      "Iteration 955: loss = 0.3677999973297119\n",
      "Iteration 956: loss = 0.3417251408100128\n",
      "Iteration 957: loss = 2.433100461959839\n",
      "Iteration 958: loss = 1.0399751663208008\n",
      "Iteration 959: loss = 4.046509265899658\n",
      "Iteration 960: loss = 0.5821585655212402\n",
      "Iteration 961: loss = 0.36791422963142395\n",
      "Iteration 962: loss = 0.3861788809299469\n",
      "Iteration 963: loss = 2.6571176052093506\n",
      "Iteration 964: loss = 0.963073194026947\n",
      "Iteration 965: loss = 4.0218586921691895\n",
      "Iteration 966: loss = 0.6042340993881226\n",
      "Iteration 967: loss = 0.3705862760543823\n",
      "Iteration 968: loss = 0.26352858543395996\n",
      "Iteration 969: loss = 0.2926551401615143\n",
      "Iteration 970: loss = 0.9321692585945129\n",
      "Iteration 971: loss = 3.4148406982421875\n",
      "Iteration 972: loss = 0.9410516619682312\n",
      "Iteration 973: loss = 4.0218892097473145\n",
      "Iteration 974: loss = 0.6026670336723328\n",
      "Iteration 975: loss = 0.3693893253803253\n",
      "Iteration 976: loss = 0.2620740234851837\n",
      "Iteration 977: loss = 0.2836831510066986\n",
      "Iteration 978: loss = 0.878332257270813\n",
      "Iteration 979: loss = 3.417131185531616\n",
      "Iteration 980: loss = 0.9283908605575562\n",
      "Iteration 981: loss = 4.017851829528809\n",
      "Iteration 982: loss = 0.6018285155296326\n",
      "Iteration 983: loss = 0.368662029504776\n",
      "Iteration 984: loss = 0.25994494557380676\n",
      "Iteration 985: loss = 0.2614089250564575\n",
      "Iteration 986: loss = 0.6997593641281128\n",
      "Iteration 987: loss = 3.4021553993225098\n",
      "Iteration 988: loss = 0.8977959752082825\n",
      "Iteration 989: loss = 4.007080554962158\n",
      "Iteration 990: loss = 0.6023597717285156\n",
      "Iteration 991: loss = 0.36889010667800903\n",
      "Iteration 992: loss = 0.2578035295009613\n",
      "Iteration 993: loss = 0.23482488095760345\n",
      "Iteration 994: loss = 0.310451477766037\n",
      "Iteration 995: loss = 2.464369773864746\n",
      "Iteration 996: loss = 1.1203467845916748\n",
      "Iteration 997: loss = 4.016186714172363\n",
      "Iteration 998: loss = 0.5626913905143738\n",
      "Iteration 999: loss = 0.36737191677093506\n"
     ]
    }
   ],
   "source": [
    "W,b = train(x_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e1278cc5-52d8-4286-bde0-ef2c07f45715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7492,  0.0161,  0.2884,  0.0088], requires_grad=True)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0d6f4713-9556-4261-8544-881f4e16019e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3892], requires_grad=True)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9c42f639-3662-4e12-9802-7a92246d53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Progagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "519612a0-5e98-42c0-869a-7f519231b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a015de9-5bd8-4538-b125-b38a95c2f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e515dfa-93b7-4337-bb98-7efc6f7bad73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2d0b9-a415-4360-81dd-51863d6283dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea21642-d342-4ba0-a545-9fa04fe35fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab969d-4c8a-40c2-b5b3-26baf16a7641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336b288-4546-4a0a-a5a7-3a05ccb007da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dcbb98-1402-4c30-8514-41429ec5e841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cc21b-b376-4ba1-b7cc-01ce7a3dc1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecd0f1-0c32-4731-ba48-cc0e52d78536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3343beb-d30f-4ba8-8db1-acd3e3aa92e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3effbd8d-db06-4836-8253-8886757e996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tested.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdfe0f1-52ce-4734-9bfd-b76fea06edf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0            892         0       3   \n",
       "1            893         1       3   \n",
       "2            894         0       2   \n",
       "3            895         0       3   \n",
       "4            896         1       3   \n",
       "..           ...       ...     ...   \n",
       "413         1305         0       3   \n",
       "414         1306         1       1   \n",
       "415         1307         0       3   \n",
       "416         1308         0       3   \n",
       "417         1309         0       3   \n",
       "\n",
       "                                             Name     Sex   Age  SibSp  Parch  \\\n",
       "0                                Kelly, Mr. James    male  34.5      0      0   \n",
       "1                Wilkes, Mrs. James (Ellen Needs)  female  47.0      1      0   \n",
       "2                       Myles, Mr. Thomas Francis    male  62.0      0      0   \n",
       "3                                Wirz, Mr. Albert    male  27.0      0      0   \n",
       "4    Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  22.0      1      1   \n",
       "..                                            ...     ...   ...    ...    ...   \n",
       "413                            Spector, Mr. Woolf    male   NaN      0      0   \n",
       "414                  Oliva y Ocana, Dona. Fermina  female  39.0      0      0   \n",
       "415                  Saether, Mr. Simon Sivertsen    male  38.5      0      0   \n",
       "416                           Ware, Mr. Frederick    male   NaN      0      0   \n",
       "417                      Peter, Master. Michael J    male   NaN      1      1   \n",
       "\n",
       "                 Ticket      Fare Cabin Embarked  \n",
       "0                330911    7.8292   NaN        Q  \n",
       "1                363272    7.0000   NaN        S  \n",
       "2                240276    9.6875   NaN        Q  \n",
       "3                315154    8.6625   NaN        S  \n",
       "4               3101298   12.2875   NaN        S  \n",
       "..                  ...       ...   ...      ...  \n",
       "413           A.5. 3236    8.0500   NaN        S  \n",
       "414            PC 17758  108.9000  C105        C  \n",
       "415  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
       "416              359309    8.0500   NaN        S  \n",
       "417                2668   22.3583   NaN        C  \n",
       "\n",
       "[418 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6de6b6e-a84f-4cf2-ba3a-e619f19c612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b8f717-12ee-490a-a9d6-23d7fffb1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('Name')\n",
    "features.remove('PassengerId')\n",
    "features.remove('Ticket')\n",
    "features.remove('Cabin')\n",
    "features.remove('Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49b903a3-3ee5-49ca-b8d8-2bd54223218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "471fa8ef-d17b-499c-92fb-f869b7f3c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('Pclass')\n",
    "features.remove('Parch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4604509b-d332-4a54-a514-3d36f88f4742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>22.3583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sex   Age  SibSp      Fare\n",
       "0      male  34.5      0    7.8292\n",
       "1    female  47.0      1    7.0000\n",
       "2      male  62.0      0    9.6875\n",
       "3      male  27.0      0    8.6625\n",
       "4    female  22.0      1   12.2875\n",
       "..      ...   ...    ...       ...\n",
       "413    male   NaN      0    8.0500\n",
       "414  female  39.0      0  108.9000\n",
       "415    male  38.5      0    7.2500\n",
       "416    male   NaN      0    8.0500\n",
       "417    male   NaN      1   22.3583\n",
       "\n",
       "[418 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58040f19-0c25-4e19-be8f-682bfec76c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ccf5c89-6beb-4f66-91a0-3f018bb4b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1152/3979549114.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Sex'] = df1['Sex'].apply(lambda x: 0 if x == \"male\" else 1)\n"
     ]
    }
   ],
   "source": [
    "df1['Sex'] = df1['Sex'].apply(lambda x: 0 if x == \"male\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa546485-a3e8-4d80-ba43-50c48fdb27ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.3583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch      Fare\n",
       "0         3    0  34.5      0      0    7.8292\n",
       "1         3    1  47.0      1      0    7.0000\n",
       "2         2    0  62.0      0      0    9.6875\n",
       "3         3    0  27.0      0      0    8.6625\n",
       "4         3    1  22.0      1      1   12.2875\n",
       "..      ...  ...   ...    ...    ...       ...\n",
       "413       3    0   NaN      0      0    8.0500\n",
       "414       1    1  39.0      0      0  108.9000\n",
       "415       3    0  38.5      0      0    7.2500\n",
       "416       3    0   NaN      0      0    8.0500\n",
       "417       3    0   NaN      1      1   22.3583\n",
       "\n",
       "[418 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "304f1b07-b394-4437-b70c-3d6c8709086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Pclass  418 non-null    int64  \n",
      " 1   Sex     418 non-null    int64  \n",
      " 2   Age     332 non-null    float64\n",
      " 3   SibSp   418 non-null    int64  \n",
      " 4   Parch   418 non-null    int64  \n",
      " 5   Fare    417 non-null    float64\n",
      "dtypes: float64(2), int64(4)\n",
      "memory usage: 19.7 KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "865617c0-fc4e-4357-8ee8-f5317ac239ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1152/3169937590.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Age'].fillna(df1['Age'].mean(),inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df1['Age'].fillna(df1['Age'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978a3619-2bde-4929-a1c9-f72aa0a6a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Pclass  418 non-null    int64  \n",
      " 1   Sex     418 non-null    int64  \n",
      " 2   Age     418 non-null    float64\n",
      " 3   SibSp   418 non-null    int64  \n",
      " 4   Parch   418 non-null    int64  \n",
      " 5   Fare    417 non-null    float64\n",
      "dtypes: float64(2), int64(4)\n",
      "memory usage: 19.7 KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e5f8abf-68b1-4740-8013-28b7cc6ef5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1152/3603713892.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Fare'].fillna(df1['Fare'].median(),inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df1['Fare'].fillna(df1['Fare'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7652c97-a1d1-4647-bda8-012b4c2b422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Pclass  418 non-null    int64  \n",
      " 1   Sex     418 non-null    int64  \n",
      " 2   Age     418 non-null    float64\n",
      " 3   SibSp   418 non-null    int64  \n",
      " 4   Parch   418 non-null    int64  \n",
      " 5   Fare    418 non-null    float64\n",
      "dtypes: float64(2), int64(4)\n",
      "memory usage: 19.7 KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfce3fa6-2dba-48c0-b1f2-ee3769c54cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8047c15-da30-4169-8792-d4b38c2d1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d0bac56-0e0e-4654-9482-a8f3953a9154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "413    0\n",
       "414    1\n",
       "415    0\n",
       "416    0\n",
       "417    0\n",
       "Name: Survived, Length: 418, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c9c611c-e398-402d-adfa-e16949c92f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = torch.tensor(X_df.to_numpy(),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67c6967b-4d2c-4f49-8286-876d2cd0ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = torch.tensor(Y_df.to_numpy(),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e39fc985-1241-4f38-933a-1a7ead3f094f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([418, 6])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eedbeada-4655-4f0d-b541-1763adf3ba30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f93fdf8-96ad-4502-8876-91f99ad7eb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([418])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "31f7c249-4c21-4d07-8980-64999fcd8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((6,1),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d5f1fd9b-55a4-42b9-8427-9e438daf7bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f72c2f-fbb5-4b5e-9158-45bab2d30553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "83e55f20-b0aa-458f-999d-66a9cda26349",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = X_df @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4fec867d-6bbc-471f-abb3-8f3290bab704",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cea1c06a-3b4c-468e-852d-72782fd6f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dot + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "61f97fdf-c7f9-412a-96fa-dcf7e214efec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1152/1022390117.py:1: UserWarning: Using a target size (torch.Size([418])) that is different to the input size (torch.Size([418, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_pred,Y_df)\n"
     ]
    }
   ],
   "source": [
    "loss = F.mse_loss(y_pred,Y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4cd2de33-2028-4dc1-93a1-14a856d444db",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m W\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m W\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GTK/DeepLearningAI/env/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GTK/DeepLearningAI/env/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "W.requires_grad = True\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a0066e79-a31f-4160-b8d2-c87428d22d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/hzr9pnz537ld_hkhgw859klm0000gn/T/ipykernel_1152/1614539626.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  W.grad\n"
     ]
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a4267a37-9863-4cb5-bb01-b7dd292a4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W - 0.01*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95fbf2df-513e-4ec3-a96a-ba0e755c9f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38c70f8a-42c3-49c0-965e-f7f0f5ba4bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.9421],\n",
       "        [  0.2403],\n",
       "        [-39.4923],\n",
       "        [  0.2070],\n",
       "        [ -0.2807],\n",
       "        [-79.6047]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b518a6-3558-4874-b80e-5f0f16f6f312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14c28521-e9df-4a33-92cb-54b48722c9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mse_loss in module torch.nn.functional:\n",
      "\n",
      "mse_loss(input: torch.Tensor, target: torch.Tensor, size_average: Optional[bool] = None, reduce: Optional[bool] = None, reduction: str = 'mean') -> torch.Tensor\n",
      "    mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\n",
      "    \n",
      "    Measures the element-wise mean squared error.\n",
      "    \n",
      "    See :class:`~torch.nn.MSELoss` for details.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17e1ded0-3012-400c-8f5d-9b3254494d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46.9952, 45.9952, 46.9952,  ..., 46.9952, 46.9952, 46.9952],\n",
       "        [62.1034, 61.1034, 62.1034,  ..., 62.1034, 62.1034, 62.1034],\n",
       "        [77.6706, 76.6706, 77.6706,  ..., 77.6706, 77.6706, 77.6706],\n",
       "        ...,\n",
       "        [51.1661, 50.1661, 51.1661,  ..., 51.1661, 51.1661, 51.1661],\n",
       "        [42.3687, 41.3687, 42.3687,  ..., 42.3687, 42.3687, 42.3687],\n",
       "        [51.4710, 50.4710, 51.4710,  ..., 51.4710, 51.4710, 51.4710]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ae58e-8ceb-4577-9e1d-af2b7b7a1545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
